{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([35, 35])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([5,7])\n",
    "\n",
    "y = torch.tensor([7,5])\n",
    "\n",
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros([2,5])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0757, 0.6286, 0.2808, 0.0340, 0.9027],\n",
       "        [0.1317, 0.2089, 0.1936, 0.7964, 0.2457]])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y =  torch.rand([2,5])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0757, 0.6286, 0.2808, 0.0340, 0.9027, 0.1317, 0.2089, 0.1936, 0.7964,\n",
       "         0.2457]])"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.view([1,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsets =  torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testsets =  torch.utils.data.DataLoader(train, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([0, 6, 5, 2, 9, 6, 4, 4, 7, 8])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainsets:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0], data[1][0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1998ee50620>"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(data[0][0].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainsets:\n",
    "    Xs , ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1\n",
    "\n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST(\"\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "trainsets =  torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "\n",
    "testsets =  torch.utils.data.DataLoader(train, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear( 28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2361, -2.2198, -2.2837, -2.3599, -2.3610, -2.3057, -2.3156, -2.1837,\n",
      "         -2.3619, -2.4240]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ouput = net(X)\n",
    "print(ouput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0964, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0079, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainsets:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.978\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainsets:   \n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))    \n",
    "        for idx, i in enumerate(output):   \n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[0].view(-1, 28*28))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetImages/Cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12501/12501 [01:15<00:00, 164.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetImages/Dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12501/12501 [01:20<00:00, 154.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats: 12476\n",
      "Dogs: 12470\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "REBUILD_DATA = False  # Set to true to run once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img, dtype=np.float32), np.eye(2)[self.LABELS[label]]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        images = np.array([i[0] for i in self.training_data])\n",
    "        labels = np.array([i[1] for i in self.training_data])\n",
    "        np.save(\"training_data_images.npy\", images)\n",
    "        np.save(\"training_data_labels.npy\", labels)\n",
    "        print('Cats:', self.catcount)\n",
    "        print('Dogs:', self.dogcount)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24946\n"
     ]
    }
   ],
   "source": [
    "training_data = np.load(\"training_data_images.npy\", allow_pickle=True)\n",
    "print(len(training_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151. 156. 156. ... 130. 125. 125.]\n",
      " [158. 162. 164. ... 139. 139. 139.]\n",
      " [161. 163. 168. ... 155. 145. 137.]\n",
      " ...\n",
      " [159. 148. 131. ... 126. 115.  98.]\n",
      " [153. 151. 130. ... 120. 123.  66.]\n",
      " [150. 144. 114. ... 121.  94.  92.]]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(training_data[0], cmap=\"gray\", aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([158., 162., 164., 162., 153., 128., 122., 130., 141., 138., 133.,\n",
       "       138., 105., 107.,  91.,  70.,  55.,  47.,  43.,  58.,  51.,  47.,\n",
       "        36.,  29.,  55.,  73.,  79.,  74.,  79.,  83.,  87.,  97., 106.,\n",
       "        92., 107., 100.,  98., 107., 108., 128., 133., 138., 134., 152.,\n",
       "       142., 148., 142., 139., 139., 139.], dtype=float32)"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Convnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "REBUILD_DATA = False # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n",
    "                        #print(np.eye(2)[self.LABELS[label]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "\n",
    "\n",
    "training_data = np.load(\"training_data_images.npy\", allow_pickle=True)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(1, 1, 50, 50)  # Use batch_size=1 for shape calculation\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)  # 2 classes\n",
    "\n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].numel()  # Compute the flattened size\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Raw logits\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Training data size: 900\n",
      "Testing data size: 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Example conversion if targets are in numpy.float32 format\n",
    "# Ensure targets are integers for classification tasks\n",
    "\n",
    "# Example dummy data\n",
    "inputs = np.random.rand(1000, 50, 50)  # Replace with your actual input data\n",
    "targets = np.random.randint(0, 2, size=(1000,))  # Dummy targets (2 classes: 0 and 1)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X = torch.tensor(inputs, dtype=torch.float32)  # Ensure data type is float32\n",
    "y = torch.tensor(targets, dtype=torch.long)    # Ensure targets are in long format for classification\n",
    "\n",
    "# Normalize inputs if needed\n",
    "X = X / 255.0\n",
    "\n",
    "# Calculate validation size\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(X) * VAL_PCT)\n",
    "print(val_size)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(f\"Training data size: {len(train_X)}\")\n",
    "print(f\"Testing data size: {len(test_X)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:00<00:01,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6924752593040466\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:00<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6882010698318481\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6923975944519043\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:00<00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6965685486793518\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6939997673034668\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:00<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6963790655136108\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.696948766708374\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6858667731285095\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.7004825472831726\n",
      "Epoch: 0. Loss: 0.7004825472831726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()  # For classification tasks\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "        batch_X = train_X[i:i+BATCH_SIZE]\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "        # Ensure correct shape\n",
    "        batch_size = batch_X.size(0)\n",
    "        if batch_X.dim() == 3:  # Check if input is missing the channel dimension\n",
    "            batch_X = batch_X.unsqueeze(1)  # Add channel dimension: [batch_size, 1, 50, 50]\n",
    "\n",
    "        print(f\"Batch X shape: {batch_X.shape}\")\n",
    "\n",
    "        net.zero_grad()\n",
    "\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        # Print output and target shapes for verification\n",
    "        print(f\"Output shape: {outputs.shape}\")\n",
    "        print(f\"Target shape: {batch_y.shape}\")\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}. Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([35, 35])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([5,7])\n",
    "\n",
    "y = torch.tensor([7,5])\n",
    "\n",
    "print(x*y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.zeros([2,5])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0757, 0.6286, 0.2808, 0.0340, 0.9027],\n",
       "        [0.1317, 0.2089, 0.1936, 0.7964, 0.2457]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y =  torch.rand([2,5])\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0757, 0.6286, 0.2808, 0.0340, 0.9027, 0.1317, 0.2089, 0.1936, 0.7964,\n",
       "         0.2457]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y.view([1,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = datasets.MNIST(\"\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainsets =  torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "testsets =  torch.utils.data.DataLoader(train, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([0, 6, 5, 2, 9, 6, 4, 4, 7, 8])]\n"
     ]
    }
   ],
   "source": [
    "for data in trainsets:\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    }
   ],
   "source": [
    "x, y = data[0][0], data[1][0]\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "print(data[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1998ee50620>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.imshow(data[0][0].view(28,28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 5923, 1: 6742, 2: 5958, 3: 6131, 4: 5842, 5: 5421, 6: 5918, 7: 6265, 8: 5851, 9: 5949}\n"
     ]
    }
   ],
   "source": [
    "total = 0\n",
    "counter_dict = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0}\n",
    "\n",
    "for data in trainsets:\n",
    "    Xs , ys = data\n",
    "    for y in ys:\n",
    "        counter_dict[int(y)] += 1\n",
    "        total += 1\n",
    "\n",
    "print(counter_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 9.871666666666666\n",
      "1: 11.236666666666666\n",
      "2: 9.93\n",
      "3: 10.218333333333334\n",
      "4: 9.736666666666666\n",
      "5: 9.035\n",
      "6: 9.863333333333333\n",
      "7: 10.441666666666666\n",
      "8: 9.751666666666667\n",
      "9: 9.915000000000001\n"
     ]
    }
   ],
   "source": [
    "for i in counter_dict:\n",
    "    print(f\"{i}: {counter_dict[i]/total*100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "train = datasets.MNIST(\"\", train=True, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "test = datasets.MNIST(\"\", train=False, download=True, transform=transforms.Compose([transforms.ToTensor()]))\n",
    "\n",
    "trainsets =  torch.utils.data.DataLoader(train, batch_size=10, shuffle=True)\n",
    "\n",
    "testsets =  torch.utils.data.DataLoader(train, batch_size=10, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc4): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear( 28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand((28,28))\n",
    "X = X.view(-1, 28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2361, -2.2198, -2.2837, -2.3599, -2.3610, -2.3057, -2.3156, -2.1837,\n",
      "         -2.3619, -2.4240]], grad_fn=<LogSoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "ouput = net(X)\n",
    "print(ouput)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0964, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0079, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0014, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for data in trainsets:\n",
    "        X, y = data\n",
    "        net.zero_grad()\n",
    "        output = net(X.view(-1, 28*28))\n",
    "        loss = F.nll_loss(output, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.978\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in trainsets:   \n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28*28))    \n",
    "        for idx, i in enumerate(output):   \n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "\n",
    "\n",
    "print(\"Accuracy: \", round(correct/total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X[0].view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "print(torch.argmax(net(X[0].view(-1, 28*28))[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetImages/Cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12501/12501 [01:15<00:00, 164.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PetImages/Dog\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12501/12501 [01:20<00:00, 154.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cats: 12476\n",
      "Dogs: 12470\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "REBUILD_DATA = False  # Set to true to run once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img, dtype=np.float32), np.eye(2)[self.LABELS[label]]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        images = np.array([i[0] for i in self.training_data])\n",
    "        labels = np.array([i[1] for i in self.training_data])\n",
    "        np.save(\"training_data_images.npy\", images)\n",
    "        np.save(\"training_data_labels.npy\", labels)\n",
    "        print('Cats:', self.catcount)\n",
    "        print('Dogs:', self.dogcount)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24946\n"
     ]
    }
   ],
   "source": [
    "training_data = np.load(\"training_data_images.npy\", allow_pickle=True)\n",
    "print(len(training_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[151. 156. 156. ... 130. 125. 125.]\n",
      " [158. 162. 164. ... 139. 139. 139.]\n",
      " [161. 163. 168. ... 155. 145. 137.]\n",
      " ...\n",
      " [159. 148. 131. ... 126. 115.  98.]\n",
      " [153. 151. 130. ... 120. 123.  66.]\n",
      " [150. 144. 114. ... 121.  94.  92.]]\n"
     ]
    }
   ],
   "source": [
    "print(training_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(training_data[0], cmap=\"gray\", aspect='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([158., 162., 164., 162., 153., 128., 122., 130., 141., 138., 133.,\n",
       "       138., 105., 107.,  91.,  70.,  55.,  47.,  43.,  58.,  51.,  47.,\n",
       "        36.,  29.,  55.,  73.,  79.,  74.,  79.,  83.,  87.,  97., 106.,\n",
       "        92., 107., 100.,  98., 107., 108., 128., 133., 138., 134., 152.,\n",
       "       142., 148., 142., 139., 139., 139.], dtype=float32)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "training_data[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Convnet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "REBUILD_DATA = False # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n",
    "                        #print(np.eye(2)[self.LABELS[label]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "\n",
    "\n",
    "training_data = np.load(\"training_data_images.npy\", allow_pickle=True)\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(1, 1, 50, 50)  # Use batch_size=1 for shape calculation\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)  # 2 classes\n",
    "\n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].numel()  # Compute the flattened size\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Raw logits\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "Training data size: 900\n",
      "Testing data size: 100\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Example conversion if targets are in numpy.float32 format\n",
    "# Ensure targets are integers for classification tasks\n",
    "\n",
    "# Example dummy data\n",
    "inputs = np.random.rand(1000, 50, 50)  # Replace with your actual input data\n",
    "targets = np.random.randint(0, 2, size=(1000,))  # Dummy targets (2 classes: 0 and 1)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X = torch.tensor(inputs, dtype=torch.float32)  # Ensure data type is float32\n",
    "y = torch.tensor(targets, dtype=torch.long)    # Ensure targets are in long format for classification\n",
    "\n",
    "# Normalize inputs if needed\n",
    "X = X / 255.0\n",
    "\n",
    "# Calculate validation size\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(X) * VAL_PCT)\n",
    "print(val_size)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(f\"Training data size: {len(train_X)}\")\n",
    "print(f\"Testing data size: {len(test_X)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:00<00:01,  7.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6924752593040466\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:00<00:00,  7.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6882010698318481\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6923975944519043\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:00<00:00,  7.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6965685486793518\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6939997673034668\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:00<00:00,  7.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6963790655136108\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.696948766708374\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  7.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6858667731285095\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.7004825472831726\n",
      "Epoch: 0. Loss: 0.7004825472831726\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize optimizer and loss function\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()  # For classification tasks\n",
    "\n",
    "# Training parameters\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "        batch_X = train_X[i:i+BATCH_SIZE]\n",
    "        batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "        # Ensure correct shape\n",
    "        batch_size = batch_X.size(0)\n",
    "        if batch_X.dim() == 3:  # Check if input is missing the channel dimension\n",
    "            batch_X = batch_X.unsqueeze(1)  # Add channel dimension: [batch_size, 1, 50, 50]\n",
    "\n",
    "        print(f\"Batch X shape: {batch_X.shape}\")\n",
    "\n",
    "        net.zero_grad()\n",
    "\n",
    "        outputs = net(batch_X)\n",
    "        loss = loss_function(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        # Print output and target shapes for verification\n",
    "        print(f\"Output shape: {outputs.shape}\")\n",
    "        print(f\"Target shape: {batch_y.shape}\")\n",
    "        print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}. Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Ensure that the network is in evaluation mode\n",
    "net.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Process the test set in batches\n",
    "    batch_size = 100\n",
    "    for i in tqdm(range(0, len(test_X), batch_size)):\n",
    "        # Slice the batch\n",
    "        batch_X = test_X[i:i+batch_size]\n",
    "        batch_y = test_y[i:i+batch_size]\n",
    "\n",
    "        # Ensure the correct shape for input data\n",
    "        if batch_X.dim() == 3:  # Check if input is missing the channel dimension\n",
    "            batch_X = batch_X.unsqueeze(1)  # Add channel dimension: [batch_size, 1, 50, 50]\n",
    "\n",
    "        # Forward pass through the network\n",
    "        outputs = net(batch_X)\n",
    "        \n",
    "        # Get the predictions\n",
    "        _, predicted_classes = torch.max(outputs, 1)\n",
    "        \n",
    "        # Compute the number of correct predictions\n",
    "        correct += (predicted_classes == batch_y).sum().item()\n",
    "        total += batch_y.size(0)\n",
    "\n",
    "# Print accuracy\n",
    "print(\"Accuracy: \", round(correct / total, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moving to GPU \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
      ")\n",
      "24946\n",
      "100\n",
      "Training data size: 900\n",
      "Testing data size: 100\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "REBUILD_DATA = False # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n",
    "                        #print(np.eye(2)[self.LABELS[label]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(1, 1, 50, 50)  # Use batch_size=1 for shape calculation\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)  # 2 classes\n",
    "\n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2, 2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].numel()  # Compute the flattened size\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)  # Flatten\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # Raw logits\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)\n",
    "\n",
    "\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "\n",
    "training_data = np.load(\"training_data_images.npy\", allow_pickle=True)\n",
    "print(len(training_data))\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "inputs = np.random.rand(1000, 50, 50)  # Replace with your actual input data\n",
    "targets = np.random.randint(0, 2, size=(1000,))  # Dummy targets (2 classes: 0 and 1)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X = torch.tensor(inputs, dtype=torch.float32)  # Ensure data type is float32\n",
    "y = torch.tensor(targets, dtype=torch.long)    # Ensure targets are in long format for classification\n",
    "\n",
    "# Normalize inputs if needed\n",
    "X = X / 255.0\n",
    "\n",
    "# Calculate validation size\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(X) * VAL_PCT)\n",
    "print(val_size)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(f\"Training data size: {len(train_X)}\")\n",
    "print(f\"Testing data size: {len(test_X)}\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "EPOCHS = 1\n",
    "\n",
    "\n",
    "def train(net):\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE]\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            # Ensure correct shape\n",
    "            batch_size = batch_X.size(0)\n",
    "            if batch_X.dim() == 3:  # Check if input is missing the channel dimension\n",
    "                batch_X = batch_X.unsqueeze(1)  # Add channel dimension: [batch_size, 1, 50, 50]\n",
    "\n",
    "            print(f\"Batch X shape: {batch_X.shape}\")\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Print output and target shapes for verification\n",
    "            print(f\"Output shape: {outputs.shape}\")\n",
    "            print(f\"Target shape: {batch_y.shape}\")\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}. Loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Ensure that the network is in evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Process the test set in batches\n",
    "        batch_size = 100\n",
    "        for i in tqdm(range(0, len(test_X), batch_size)):\n",
    "            # Slice the batch\n",
    "            batch_X = test_X[i:i+batch_size]\n",
    "            batch_y = test_y[i:i+batch_size]\n",
    "\n",
    "            # Ensure the correct shape for input data\n",
    "            if batch_X.dim() == 3:  # Check if input is missing the channel dimension\n",
    "                batch_X = batch_X.unsqueeze(1)  # Add channel dimension: [batch_size, 1, 50, 50]\n",
    "\n",
    "            # Forward pass through the network\n",
    "            outputs = net(batch_X)\n",
    "        \n",
    "            # Get the predictions\n",
    "            _, predicted_classes = torch.max(outputs, 1)\n",
    "        \n",
    "            # Compute the number of correct predictions\n",
    "            correct += (predicted_classes == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "\n",
    "    # Print accuracy\n",
    "    print(\"Accuracy: \", round(correct / total, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:00<00:01,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6927664875984192\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6972547173500061\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:00<00:00,  5.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6892587542533875\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6915618181228638\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:01<00:00,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6952067613601685\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.695953905582428\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:01<00:00,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6871296167373657\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6899547576904297\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.7021464705467224\n",
      "Epoch: 0. Loss: 0.7021464705467224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (fc2): Linear(in_features=512, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:00<00:01,  5.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6891078948974609\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:00<00:02,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.695498526096344\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:04<00:12,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6886647939682007\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:12<00:21,  4.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6916133165359497\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:15<00:15,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6945967078208923\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:20<00:12,  4.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.695295512676239\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:22<00:07,  3.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6874765753746033\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:25<00:03,  3.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.690207839012146\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:28<00:00,  3.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.7006392478942871\n",
      "Epoch: 0. Loss: 0.7006392478942871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def train(net):\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].to(device)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE].to(device)\n",
    "\n",
    "            # Ensure correct shape\n",
    "            batch_size = batch_X.size(0)\n",
    "            if batch_X.dim() == 3:  # Check if input is missing the channel dimension\n",
    "                batch_X = batch_X.unsqueeze(1)  # Add channel dimension: [batch_size, 1, 50, 50]\n",
    "\n",
    "            print(f\"Batch X shape: {batch_X.shape}\")\n",
    "\n",
    "            net.zero_grad()\n",
    "\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()  # Update weights\n",
    "\n",
    "            # Print output and target shapes for verification\n",
    "            print(f\"Output shape: {outputs.shape}\")\n",
    "            print(f\"Target shape: {batch_y.shape}\")\n",
    "            print(f\"Loss: {loss.item()}\")\n",
    "\n",
    "    print(f\"Epoch: {epoch}. Loss: {loss.item()}\")\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:00<00:01,  5.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.689379870891571\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n",
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6952686905860901\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:00<00:01,  3.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6889073848724365\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:05<00:09,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6916666626930237\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:08<00:09,  2.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6944779753684998\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:12<00:09,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6951534748077393\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:18<00:08,  4.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6877223253250122\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:22<00:03,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.6903088092803955\n",
      "Batch X shape: torch.Size([100, 1, 50, 50])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:26<00:00,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([100, 2])\n",
      "Target shape: torch.Size([100])\n",
      "Loss: 0.7003127336502075\n",
      "Epoch: 0. Loss: 0.7003127336502075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 17.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # Ensure that the network is in evaluation mode\n",
    "    net.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Process the test set in batches\n",
    "        batch_size = 100\n",
    "        for i in tqdm(range(0, len(test_X), batch_size)):\n",
    "            # Slice the batch\n",
    "            batch_X = test_X[i:i+batch_size].to(device)\n",
    "            batch_y = test_y[i:i+batch_size].to(device)\n",
    "\n",
    "            # Ensure the correct shape for input data\n",
    "            if batch_X.dim() == 3:  # Check if input is missing the channel dimension\n",
    "                batch_X = batch_X.unsqueeze(1)  # Add channel dimension: [batch_size, 1, 50, 50]\n",
    "\n",
    "            # Forward pass through the network\n",
    "            outputs = net(batch_X)\n",
    "        \n",
    "            # Get the predictions\n",
    "            _, predicted_classes = torch.max(outputs, 1)\n",
    "        \n",
    "            # Compute the number of correct predictions\n",
    "            correct += (predicted_classes == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "\n",
    "    # Print accuracy\n",
    "    print(\"Accuracy: \", round(correct / total, 3))\n",
    "\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Model Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on the CPU\n",
      "24946\n",
      "100\n",
      "Training data size: 900\n",
      "Testing data size: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  5.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6940, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  7.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6942, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:01<00:00,  6.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6942, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 651.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "REBUILD_DATA = False # set to true to one once, then back to false unless you want to change something in your training data.\n",
    "\n",
    "class DogsVSCats():\n",
    "    IMG_SIZE = 50\n",
    "    CATS = \"PetImages/Cat\"\n",
    "    DOGS = \"PetImages/Dog\"\n",
    "    TESTING = \"PetImages/Testing\"\n",
    "    LABELS = {CATS: 0, DOGS: 1}\n",
    "    training_data = []\n",
    "\n",
    "    catcount = 0\n",
    "    dogcount = 0\n",
    "\n",
    "    def make_training_data(self):\n",
    "        for label in self.LABELS:\n",
    "            print(label)\n",
    "            for f in tqdm(os.listdir(label)):\n",
    "                if \"jpg\" in f:\n",
    "                    try:\n",
    "                        path = os.path.join(label, f)\n",
    "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "                        img = cv2.resize(img, (self.IMG_SIZE, self.IMG_SIZE))\n",
    "                        self.training_data.append([np.array(img), np.eye(2)[self.LABELS[label]]])  # do something like print(np.eye(2)[1]), just makes one_hot \n",
    "                        #print(np.eye(2)[self.LABELS[label]])\n",
    "\n",
    "                        if label == self.CATS:\n",
    "                            self.catcount += 1\n",
    "                        elif label == self.DOGS:\n",
    "                            self.dogcount += 1\n",
    "\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "                        #print(label, f, str(e))\n",
    "\n",
    "        np.random.shuffle(self.training_data)\n",
    "        np.save(\"training_data.npy\", self.training_data)\n",
    "        print('Cats:',dogsvcats.catcount)\n",
    "        print('Dogs:',dogsvcats.dogcount)\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "\n",
    "        x = torch.randn(50,50).view(-1,1,50,50)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "\n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 2)\n",
    "\n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))\n",
    "\n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "    print(\"Running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "net = Net().to(device)\n",
    "\n",
    "if REBUILD_DATA:\n",
    "    dogsvcats = DogsVSCats()\n",
    "    dogsvcats.make_training_data()\n",
    "\n",
    "training_data = np.load(\"training_data_images.npy\", allow_pickle=True)\n",
    "print(len(training_data))\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "inputs = np.random.rand(1000, 50, 50)  # Replace with your actual input data\n",
    "targets = np.random.randint(0, 2, size=(1000,))  # Dummy targets (2 classes: 0 and 1)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "X = torch.tensor(inputs, dtype=torch.float32)  # Ensure data type is float32\n",
    "y = torch.tensor(targets, dtype=torch.long)    # Ensure targets are in long format for classification\n",
    "\n",
    "# Normalize inputs if needed\n",
    "X = X / 255.0\n",
    "\n",
    "# Calculate validation size\n",
    "VAL_PCT = 0.1\n",
    "val_size = int(len(X) * VAL_PCT)\n",
    "print(val_size)\n",
    "\n",
    "train_X = X[:-val_size]\n",
    "train_y = y[:-val_size]\n",
    "\n",
    "test_X = X[-val_size:]\n",
    "test_y = y[-val_size:]\n",
    "\n",
    "print(f\"Training data size: {len(train_X)}\")\n",
    "print(f\"Testing data size: {len(test_X)}\")\n",
    "\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 3\n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X)\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(loss)\n",
    "\n",
    "def test(net):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(test_X))):\n",
    "            real_class = torch.argmax(test_y[i]).to(device)\n",
    "            net_out = net(test_X[i].view(-1, 1, 50, 50).to(device))[0]\n",
    "\n",
    "            predicted_class = torch.argmax(net_out)\n",
    "            if predicted_class == real_class:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    print(\"Accuracy:\", round(correct/total,3))\n",
    "\n",
    "train(net)\n",
    "test(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fwd_pass(X, y, train=False):\n",
    "    if train:\n",
    "        net.zero_grad()\n",
    "    outputs = net(X)\n",
    "    matches = [torch.argmax(i) == torch.argmax(j) for i, j in zip(outputs, y)]\n",
    "    acc = matches.count(True)/len(matches)\n",
    "    loss = loss_function(outputs, y)\n",
    "\n",
    "    if train:\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return acc, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 tensor(0.6934)\n"
     ]
    }
   ],
   "source": [
    "def test(size=32):\n",
    "    random_start = np.random.randint(len(test_X) - size)\n",
    "    X, y = test_X[random_start:random_start+size], test_y[random_start:random_start+size]\n",
    "    with torch.no_grad():\n",
    "        val_acc, val_loss = fwd_pass(X.view(-1, 1, 50, 50).to(device), y.to(device))\n",
    "    return val_acc, val_loss\n",
    "\n",
    "val_acc, val_loss = test(size=32)\n",
    "print(val_acc, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (100) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[245], line 29\u001b[0m\n\u001b[0;32m     26\u001b[0m                     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     27\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[245], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(net)\u001b[0m\n\u001b[0;32m     16\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m train_y[i:i\u001b[38;5;241m+\u001b[39mBATCH_SIZE]\n\u001b[0;32m     18\u001b[0m batch_X, batch_y \u001b[38;5;241m=\u001b[39m batch_X\u001b[38;5;241m.\u001b[39mto(device), batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 20\u001b[0m acc, loss \u001b[38;5;241m=\u001b[39m \u001b[43mfwd_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m#print(f\"Acc: {round(float(acc),2)}  Loss: {round(float(loss),4)}\")\u001b[39;00m\n\u001b[0;32m     23\u001b[0m f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(time\u001b[38;5;241m.\u001b[39mtime(),\u001b[38;5;241m3\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,in_sample,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(acc),\u001b[38;5;241m2\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(loss),\u001b[38;5;241m4\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[187], line 7\u001b[0m, in \u001b[0;36mfwd_pass\u001b[1;34m(X, y, train)\u001b[0m\n\u001b[0;32m      5\u001b[0m matches \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39margmax(i) \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(j) \u001b[38;5;28;01mfor\u001b[39;00m i, j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(outputs, y)]\n\u001b[0;32m      6\u001b[0m acc \u001b[38;5;241m=\u001b[39m matches\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(matches)\n\u001b[1;32m----> 7\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mloss_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m train:\n\u001b[0;32m     10\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\modules\\loss.py:535\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 535\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\nn\\functional.py:3365\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3363\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3365\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3366\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (100) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "MODEL_NAME = f\"model-{int(time.time())}\"  # gives a dynamic model name, to just help with things getting messy over time. \n",
    "net = Net().to(device)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "loss_function = nn.MSELoss()\n",
    "\n",
    "def train(net):\n",
    "    BATCH_SIZE = 100\n",
    "    EPOCHS = 7\n",
    "\n",
    "    with open(\"model.log\", \"a\") as f:\n",
    "        for epoch in range(EPOCHS):\n",
    "            for i in tqdm(range(0, len(train_X), BATCH_SIZE)):\n",
    "                batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,50,50)\n",
    "                batch_y = train_y[i:i+BATCH_SIZE]\n",
    "\n",
    "                batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "                acc, loss = fwd_pass(batch_X, batch_y, train=True)\n",
    "\n",
    "                #print(f\"Acc: {round(float(acc),2)}  Loss: {round(float(loss),4)}\")\n",
    "                f.write(f\"{MODEL_NAME},{round(time.time(),3)},in_sample,{round(float(acc),2)},{round(float(loss),4)}\\n\")\n",
    "                # just to show the above working, and then get out:\n",
    "                if i == 5:\n",
    "                    break\n",
    "                break\n",
    "\n",
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "\n",
    "style.use(\"ggplot\")\n",
    "\n",
    "model_name = MODEL_NAME\n",
    "\n",
    "def create_acc_loss_graph(model_name):\n",
    "    contents = open(\"model.log\", \"r\").read().split(\"\\n\")\n",
    "\n",
    "    times = []\n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    val_accs = []\n",
    "    val_losses = []\n",
    "\n",
    "    for c in contents:\n",
    "        if model_name in c:\n",
    "            name, timestamp, acc, loss, val_acc, val_loss, epoch = c.split(\",\")\n",
    "\n",
    "            times.append(float(timestamp))\n",
    "            accuracies.append(float(acc))\n",
    "            losses.append(float(loss))\n",
    "\n",
    "            val_accs.append(float(val_acc))\n",
    "            val_losses.append(float(val_loss))\n",
    "\n",
    "    fig = plt.figure()\n",
    "\n",
    "    ax1 = plt.subplot2grid((2,1), (0,0))\n",
    "    ax2 = plt.subplot2grid((2,1), (1,0), sharex=ax1)\n",
    "\n",
    "    ax1.plot(times, accuracies, label=\"acc\")\n",
    "    ax1.plot(times, val_accs, label=\"val_acc\")\n",
    "    ax1.legend(loc=2)\n",
    "\n",
    "    ax2.plot(times, losses, label=\"loss\")\n",
    "    ax2.plot(times, val_losses, label=\"val_loss\")\n",
    "    ax2.legend(loc=2)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
